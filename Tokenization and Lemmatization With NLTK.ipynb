{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import MWETokenizer \n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Convolutional Neural Networks (CNN) is a deep neural networkbased learning architecture for processing a massive amount of data.',\n",
       " 'Nowadays, it is widely applied for medical imaging analysis.',\n",
       " 'The CNN is used extensively over other Machine Learning methods because it does not need any manual feature extraction as well as does not require specific segmentation.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sentences Tokenizer\n",
    "text=\"Convolutional Neural Networks (CNN) is a deep neural networkbased learning architecture for processing a massive amount of data. Nowadays, it is widely applied for medical imaging analysis. The CNN is used extensively over other Machine Learning methods because it does not need any manual feature extraction as well as does not require specific segmentation.\"\n",
    "sentences=sent_tokenize(text)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Christian', 'Charles', 'Philip', 'Bale', 'is', 'an', 'English', 'actor', 'and', 'considered', 'to', 'be', 'one', 'of', 'the', 'finest', 'actors', 'of', 'his', 'generation', '.']\n"
     ]
    }
   ],
   "source": [
    "#Word Tokenizer\n",
    "text =\"Christian Charles Philip Bale is an English actor and considered to be one of the finest actors of his generation.\"\n",
    "words=word_tokenize(text)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Convolutional_Neural_Networks', '(CNN)', 'is', 'a', 'deep', 'neural', 'networkbased', 'learning', 'architecture', 'for', 'processing', 'a', 'massive', 'amount_of', 'data.']\n"
     ]
    }
   ],
   "source": [
    "#Multiword Tokenizer\n",
    "tk = MWETokenizer([ ('Convolutional','Neural','Networks')]) \n",
    "tk.add_mwe(('amount','of'))\n",
    "text=\"Convolutional Neural Networks (CNN) is a deep neural networkbased learning architecture for processing a massive amount of data.\" \n",
    "mwtoken = tk.tokenize(text.split()) \n",
    "   \n",
    "print(mwtoken) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "study\n",
      "leaf\n",
      "decrease\n",
      "play\n",
      "study\n",
      "studying\n"
     ]
    }
   ],
   "source": [
    "#Lemmatization\n",
    "lemma=WordNetLemmatizer()\n",
    "word_list=[\"studies\",\"leaves\",\"decreases\",\"plays\"]\n",
    "\n",
    "for w in word_list:\n",
    "    print(lemma.lemmatize(w))\n",
    "    \n",
    "#PoS and Lemmatizaiton\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize('studying',pos=\"v\"))\n",
    "print(lemmatizer.lemmatize('studying',pos=\"n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harry Potter is a series of seven fantasy novel written by British author J. K. Rowling .\n"
     ]
    }
   ],
   "source": [
    "sentence='Harry Potter is a series of seven fantasy novels written by British author J. K. Rowling.'\n",
    "word_list = nltk.word_tokenize(sentence)\n",
    "\n",
    "lemmatized_output = ' '.join([lemmatizer.lemmatize(w) for w in word_list])\n",
    "print(lemmatized_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Harry', 'Potter', 'be', 'a', 'series', 'of', 'seven', 'fantasy', 'novel', 'write', 'by', 'British', 'author', 'J.', 'K.', 'Rowling', '.']\n"
     ]
    }
   ],
   "source": [
    "def get_pos(word):\n",
    "   \n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "sentence='Harry Potter is a series of seven fantasy novels written by British author J. K. Rowling.'\n",
    "print([lemmatizer.lemmatize(w, get_pos(w)) for w in nltk.word_tokenize(sentence)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
